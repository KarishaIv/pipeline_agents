import asyncio
import json
import pandas as pd
import numpy as np
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Callable
import logging

from src.data_loading import load_synthetic_data, load_american_data, preprocess_pgm_data, load_survey_data, load_evidence_from_json
from src.pgm_model import create_pgm_model, train_pgm_model, generate_synthetic_data
from src.utils import normalize_features, normalize_evidence, filter_real_russian_data, translate_ocean_to_readable, get_income_range
from src.clustering import replicate_personas_with_gmm
from src.core.simulation_manager import SimulationManager
from src.core.storage import StorageManager

from config import *
from pandarallel import pandarallel
import pandas as pd
pandarallel.initialize(progress_bar=False, nb_workers=4) 

logger = logging.getLogger(__name__)

class PipelineRunner:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–∏–º—É–ª—è—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω"""
    
    def __init__(self, config: dict):
        self.config = config
        self.output_dir = Path(config['output'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.use_pgm = config.get('use_pgm', True)
        
    async def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        logger.info("üöÄ Starting pipeline execution")
        logger.info(f"Mode: {'PGM (synthetic)' if self.use_pgm else 'Real data (filtered)'}")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        datasets = await self._load_datasets()
        
        # 2. –û–±—É—á–µ–Ω–∏–µ PGM –º–æ–¥–µ–ª–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º PGM)
        pgm_model = await self._train_pgm_model(datasets['russian']) if self.use_pgm else None
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è/—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω –¥–ª—è –≤—Å–µ—Ö —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π
        all_personas = await self._generate_or_filter_personas(datasets, pgm_model)
        
        # 4. –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–π
        results = await self._run_simulations(all_personas, datasets)
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        await self._save_results(all_personas, results, datasets)
        
        logger.info("‚úÖ Pipeline completed successfully")
        return results
    
    async def _load_datasets(self) -> Dict[str, any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö datasets"""
        logger.info("Loading datasets...")

        evidence_data = load_evidence_from_json(self.config['evidence'])
        nemo_data = load_american_data(self.config['nemo_size'])
        russian_data = load_synthetic_data()
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
        russian_data_preprocessed = preprocess_pgm_data(russian_data)
        
        datasets = {
            'evidence': evidence_data,
            'nemo': nemo_data,
            'russian': russian_data,
            'russian_preprocessed': russian_data_preprocessed
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–ø—Ä–æ—Å–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.config['agent_mode'] == 'survey':
            try:
                datasets['survey_questions'] = load_survey_data()
                logger.info(f"Loaded {len(datasets['survey_questions'])} survey questions")
            except Exception as e:
                logger.warning(f"Failed to load survey questions: {e}")
                datasets['survey_questions'] = []
        
        logger.info(f"  ‚úì Evidence: {len(evidence_data)} target audiences")
        logger.info(f"  ‚úì Russian data: {len(russian_data)} personas")
        logger.info(f"  ‚úì American data: {len(nemo_data)} personas")
        logger.info(f"  ‚úì Using {'PGM generation' if self.use_pgm else 'real data filtering'}")
        
        return datasets
    
    async def _train_pgm_model(self, russian_data: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ PGM –º–æ–¥–µ–ª–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ PGM)"""
        if not self.use_pgm:
            logger.info("‚è≠Ô∏è  Skipping PGM training (use_pgm=False)")
            return None
            
        logger.info("üß† Training PGM model...")
        
        df_prep = preprocess_pgm_data(russian_data)
        model = create_pgm_model()
        trained_model = train_pgm_model(model, df_prep)
        
        logger.info(f"  ‚úì Model trained: {len(trained_model.nodes())} nodes, {len(trained_model.edges())} edges")
        return trained_model
    
    async def _generate_or_filter_personas(self, datasets: Dict, pgm_model) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω –¥–ª—è –≤—Å–µ—Ö —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π"""
        if self.use_pgm:
            logger.info("üë• Generating synthetic personas via PGM...")
            all_personas, ta_summary = await generate_personas_via_pgm(
                evidence_list=datasets['evidence'],
                model=pgm_model,
                df_base=datasets['russian'],
                nemo=datasets['nemo'],
                output_dir=self.output_dir,
                ta_concurrency=self.config['ta_concurrency'],
                ocean_flag=self.config['ocean_flag']
            )
        else:
            logger.info("üîç Filtering real Russian personas by evidence...")
            all_personas, ta_summary = await filter_personas_from_real_data(
                evidence_list=datasets['evidence'],
                df_russian_preprocessed=datasets['russian_preprocessed'],
                nemo=datasets['nemo'],
                output_dir=self.output_dir,
                ta_concurrency=self.config['ta_concurrency'],
                ocean_flag=self.config['ocean_flag']
            )

        if 'age_group' in all_personas.columns:
            all_personas['age_group'] = all_personas['age_group'].apply(
                lambda x: f'{int(x)*5}-{int(x)*5+4}' if isinstance(x, (int, float)) else str(x)
            )

        
        logger.info(f"  ‚úì Processed {len(all_personas)} personas across {len(datasets['evidence'])} target audiences")
        return all_personas
    
    async def _run_simulations(self, all_personas: pd.DataFrame, datasets):
        """–ó–∞–ø—É—Å–∫ –º—É–ª—å—Ç–∏-–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π"""
        logger.info("ü§ñ Running multi-agent simulations...")
        
        personas = [all_personas.iloc[i].to_dict() for i in range(len(all_personas))]
        
        manager = SimulationManager(
            out_dir=self.output_dir,
            concurrency=self.config['concurrency'],
            timeout=self.config['timeout'],
            visualize=(self.config['agent_mode'] == 'credit'),
            run_retries=1,
            agent_mode=self.config['agent_mode'],
            survey_questions=datasets.get('survey_questions', [])
        )
        
        timestamp = datetime.now().strftime("%m%d_%H%M%S")
        results = await manager.run_many(
            personas, 
            steps=self.config['simulation_steps'], 
            out_subdir=f"sim_{timestamp}"
        )
        
        logger.info(f"  ‚úì Completed {len(results)} simulations in {self.config['agent_mode']} mode")
        return results
    
    async def _save_results(self, all_personas: pd.DataFrame, results: List, datasets: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        logger.info("üíæ Saving pipeline results...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        personas_path = self.output_dir / f'all_replicated_personas_{timestamp}.csv'
        all_personas.to_csv(personas_path, index=False)
        
        for i, persona in enumerate([all_personas.iloc[i].to_dict() for i in range(len(all_personas))]):
            await StorageManager.save_json_async(
                persona, 
                self.output_dir / f"profile_{i}_{timestamp}.json"
            )
        
        summary = {
            'timestamp': timestamp,
            'total_target_audiences': len(datasets['evidence']),
            'total_personas_generated': len(all_personas),
            'total_simulations_completed': len(results),
            'pipeline_mode': 'pgm' if self.use_pgm else 'real_data',
            'pipeline_config': self.config,
            'output_files': {
                'personas': str(personas_path),
                'profiles': f"profile_*_{timestamp}.json",
                'simulations': f"sim_{timestamp}/"
            }
        }
        
        summary_path = self.output_dir / f'pipeline_summary_{timestamp}.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  üìä Summary report: {summary_path}")
        logger.info(f"  üë• Personas data: {personas_path}")


async def generate_personas_via_pgm(
    evidence_list: List[Dict],
    model,
    df_base: pd.DataFrame,
    nemo: pd.DataFrame,
    output_dir: Path,
    ta_concurrency: int = 2,
    ocean_flag: bool = True
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω —á–µ—Ä–µ–∑ PGM –º–æ–¥–µ–ª—å"""
    
    async def data_fetcher(evidence: Dict, ta_index: int) -> Tuple[pd.DataFrame, str, int, str]:
        """–§–µ—Ç—á–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è PGM —Ä–µ–∂–∏–º–∞"""
        ta_name = evidence.get('target_audience_name', f'TA_{ta_index}')
        synthetic_size = evidence.get('synthetic_size', 10)
        
        logger.info(f"[TA:{ta_name}] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {synthetic_size} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–µ—Ä—Å–æ–Ω —á–µ—Ä–µ–∑ PGM")
        
        normalized_evidence = normalize_evidence(evidence)
        synthetic_data = generate_synthetic_data(
            model, evidence=normalized_evidence, size=synthetic_size
        )
        return synthetic_data, ta_name, synthetic_size, 'pgm_synthetic'
    
    return await _process_target_audiences_generic(
        evidence_list=evidence_list,
        data_fetcher=data_fetcher,
        nemo=nemo,
        output_dir=output_dir,
        ta_concurrency=ta_concurrency,
        ocean_flag=ocean_flag
    )


async def filter_personas_from_real_data(
    evidence_list: List[Dict],
    df_russian_preprocessed: pd.DataFrame,
    nemo: pd.DataFrame,
    output_dir: Path,
    ta_concurrency: int = 2,
    ocean_flag: bool = True
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø–µ—Ä—Å–æ–Ω –ø–æ evidence"""
    
    async def data_fetcher(evidence: Dict, ta_index: int) -> Tuple[pd.DataFrame, str, int, str]:
        """–§–µ—Ç—á–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∂–∏–º–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        ta_name = evidence.get('target_audience_name', f'TA_{ta_index}')
        sample_size = evidence.get('synthetic_size', 10)
        
        logger.info(f"[TA:{ta_name}] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω")
        
        filtered_data = filter_real_russian_data(evidence, df_russian_preprocessed, sample_size)
        
        if len(filtered_data) == 0:
            logger.warning(f"[TA:{ta_name}] –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏")
            empty_data = pd.DataFrame(columns=df_russian_preprocessed.columns)
            return empty_data, ta_name, sample_size, 'real_filtered'
        
        return filtered_data, ta_name, len(filtered_data), 'real_filtered'
    
    return await _process_target_audiences_generic(
        evidence_list=evidence_list,
        data_fetcher=data_fetcher,
        nemo=nemo,
        output_dir=output_dir,
        ta_concurrency=ta_concurrency,
        ocean_flag=ocean_flag
    )


async def _process_target_audiences_generic(
    evidence_list: List[Dict],
    data_fetcher: Callable,
    nemo: pd.DataFrame,
    output_dir: Path,
    ta_concurrency: int = 2,
    ocean_flag: bool = True
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    –û–±—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π
    
    Args:
        evidence_list: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π
        data_fetcher: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (PGM –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö)
        nemo: –ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        ta_concurrency: –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¶–ê
    
    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: (–≤—Å–µ –ø–µ—Ä—Å–æ–Ω—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¶–ê)
    """
    
    async def process_target_audience(ta_index: int, evidence: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ - –æ–±—â–∞—è –ª–æ–≥–∏–∫–∞"""
        # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        russian_data, ta_name, original_size, data_source = await data_fetcher(evidence, ta_index)
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if len(russian_data) == 0:
            logger.warning(f"[TA:{ta_name}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è features 
        russian_norm, american_norm = normalize_features(russian_data, nemo)

        if ocean_flag:
        
            # 4. GMM –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è 
            replicated_personas, clustering_stats = replicate_personas_with_gmm(
                russian_df=russian_data,
                russian_norm=russian_norm,
                american_norm=american_norm,
                nemo_full=nemo.copy(),
                group_cols=MATCH_COLS,
                top_n_categories=TOP_N_CATEGORIES,
                gmm_params=GMM_CONFIG,
                use_gower=False,
                sampling_method='gmm_sample',
                ta_name=ta_name
            )

            final_personas = translate_ocean_to_readable(replicated_personas)

        else:
            final_personas = russian_data
           
        # 5. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö 
        final_personas = _add_metadata_to_personas(
            personas=final_personas,
            ta_index=ta_index,
            ta_name=ta_name,
            data_source=data_source
        )
        
        # 6. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞—â–µ–Ω–∏–µ–º
        unique_clusters = None
        unique_groups = None
        if ocean_flag and len(final_personas) > 0:
            if 'cluster_id' in final_personas.columns:
                unique_clusters = final_personas['cluster_id'].nunique()
            if 'group_key' in final_personas.columns:
                unique_groups = final_personas['group_key'].nunique()
        
        ta_stats = _create_ta_stats(
            ta_index=ta_index,
            ta_name=ta_name,
            data_source=data_source,
            original_size=original_size,
            replicated_size=len(final_personas),
            unique_clusters=unique_clusters,
            unique_groups=unique_groups
        )

        logger.info(f"[TA:{ta_name}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(final_personas)} –ø–µ—Ä—Å–æ–Ω")
        return final_personas, ta_stats
    
    return await _process_target_audiences_parallel(
        evidence_list, process_target_audience, output_dir, ta_concurrency
    )

def _add_metadata_to_personas(
    personas: pd.DataFrame,
    ta_index: int,
    ta_name: str,
    data_source: str
) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –ø–µ—Ä—Å–æ–Ω–∞–º"""
    personas = personas.copy()
    personas['target_audience_id'] = ta_index
    personas['target_audience_name'] = ta_name
    personas['data_source'] = data_source
    return personas


def _create_ta_stats(
    ta_index: int,
    ta_name: str,
    data_source: str,
    original_size: int,
    replicated_size: int,
    unique_clusters: int,
    unique_groups: int
) -> Dict:
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"""
    return {
        'target_audience_id': ta_index,
        'target_audience_name': ta_name,
        'data_source': data_source,
        'original_size': original_size,
        'replicated_size': replicated_size,
        'unique_clusters': unique_clusters,
        'unique_groups': unique_groups
    }


async def _process_target_audiences_parallel(
    evidence_list: List[Dict],
    process_function: Callable,
    output_dir: Path,
    ta_concurrency: int = 2
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """–û–±—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π"""
    
    logger.info(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(evidence_list)} —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π (–ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: {ta_concurrency})")
    
    semaphore = asyncio.Semaphore(ta_concurrency)
    
    async def process_with_semaphore(ta_index, evidence):
        async with semaphore:
            return await process_function(ta_index, evidence)
    
    tasks = [
        process_with_semaphore(i, evidence) 
        for i, evidence in enumerate(evidence_list)
    ]
    
    results = await asyncio.gather(*tasks)
    all_personas_list, all_stats_list = zip(*results)
    
    all_personas = pd.concat(all_personas_list, ignore_index=True)
    ta_summary_stats = pd.DataFrame(all_stats_list)
    
    ta_summary_stats.to_csv(output_dir / 'target_audiences_summary.csv', index=False)
    total_personas = len(all_personas)
    total_tas = len(evidence_list)
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_personas} –ø–µ—Ä—Å–æ–Ω —á–µ—Ä–µ–∑ {total_tas} —Ü–µ–ª–µ–≤—ã—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–π")
    
    for _, row in ta_summary_stats.iterrows():
        logger.info(f"  ‚Ä¢ {row['target_audience_name']}: {row['replicated_size']} –ø–µ—Ä—Å–æ–Ω "
                   f"({row['unique_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤) - {row['data_source']}")
    
    return all_personas, ta_summary_stats